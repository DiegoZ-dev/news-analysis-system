{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12396e7",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [11]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ccfe27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:58:52.382039Z",
     "iopub.status.busy": "2024-08-27T17:58:52.381572Z",
     "iopub.status.idle": "2024-08-27T17:58:53.084381Z",
     "shell.execute_reply": "2024-08-27T17:58:53.083547Z"
    },
    "papermill": {
     "duration": 0.711675,
     "end_time": "2024-08-27T17:58:53.086028",
     "exception": false,
     "start_time": "2024-08-27T17:58:52.374353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6150925c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:58:53.090812Z",
     "iopub.status.busy": "2024-08-27T17:58:53.090514Z",
     "iopub.status.idle": "2024-08-27T17:58:59.566122Z",
     "shell.execute_reply": "2024-08-27T17:58:59.564955Z"
    },
    "papermill": {
     "duration": 6.480747,
     "end_time": "2024-08-27T17:58:59.568731",
     "exception": false,
     "start_time": "2024-08-27T17:58:53.087984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 14:58:54 WARN Utils: Your hostname, mauro-thinkpad resolves to a loopback address: 127.0.1.1; using 192.168.0.212 instead (on interface wlp0s20f3)\n",
      "24/08/27 14:58:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 14:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 14:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"NewsDataInspection\").getOrCreate()\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "file_path = '../data/raw/news.csv'\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5bf84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:58:59.578744Z",
     "iopub.status.busy": "2024-08-27T17:58:59.578348Z",
     "iopub.status.idle": "2024-08-27T17:59:00.058984Z",
     "shell.execute_reply": "2024-08-27T17:59:00.058450Z"
    },
    "papermill": {
     "duration": 0.486967,
     "end_time": "2024-08-27T17:59:00.060144",
     "exception": false,
     "start_time": "2024-08-27T17:58:59.573177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensiones del DataFrame: 1917 filas, 3 columnas\n"
     ]
    }
   ],
   "source": [
    "# Contar el número de filas y columnas\n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(f\"\\nDimensiones del DataFrame: {num_rows} filas, {num_cols} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e6d093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:00.065225Z",
     "iopub.status.busy": "2024-08-27T17:59:00.065011Z",
     "iopub.status.idle": "2024-08-27T17:59:00.289557Z",
     "shell.execute_reply": "2024-08-27T17:59:00.288868Z"
    },
    "papermill": {
     "duration": 0.228542,
     "end_time": "2024-08-27T17:59:00.290790",
     "exception": false,
     "start_time": "2024-08-27T17:59:00.062248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|titulo                                                                                                        |link                                                                                                                                                   |cuerpo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Gamescom 2024: vuelve Indiana Jones en una nueva aventura y llega Borderlands 4                               |https://www.lanacion.com.ar/tecnologia/gamescom-2024-vuelve-indiana-jones-en-una-nueva-aventura-y-llega-borderlands-4-nid21082024/                     |Borderlands 4 el próximo año, pero antes, el 9 de diciembre lo hará Indiana Jones y el Gran Círculo, como se ha conocido en la ceremonia inaugural de la feria de videojuegos Gamescom, que se ha celebrado este martes por la noche. Firmas del sector como 2K y Xbox, y otras más recientes como Netflix y Amazon, han compartido algunas novedades de sus títulos durante la Gamescom Opening Night Live, presentada por el periodista especializado Geoff Keighley. 2K ha anunciado una nueva entrega de Borderlands, el cuarto título principal y séptimo de la franquicia. Estará disponible el próximo año para PlayStation 5, Xbox Series X|S y PC a través de Steam y Epic Games Store. El juego de estrategia Sid Meier’s Civilization VII estará disponible en todo el mundo el 11 de febrero, con soporte para juego cruzado y progresión cruzada, en PlayStation 4 y 5, Xbox Series X|S, Xbox One, Nintendo Switch y PC a través de Steam y Epic Games Store. Este título tiene como narradora a la actriz Gwendoline Christie e incorpora características como las Eras y la posibilidad de seleccionar líderes y civilizaciones de forma independiente para mezclar y combinar bonificaciones de partida. Mafia: The Old Country (2025) llevará a los jugadores al inframundo de la Sicilia de los años 1900, con una historia que permitirá descubrir los orígenes del crimen organizado. Llegará a PlayStation 5, Xbox Series X|S y PC a través de Steam. Xbox, por su parte, ha confirmado la fecha de lanzamiento de Indiana Jones y el Gran Círculo, que llegará primero a Xbox Series X|S y PC el 9 de diciembre, y en primavera del próximo año a PlayStation 5. La firma tecnológica también ha mostrado trailers de Age of Mythology: Retold (4 septiembre), la misión de campaña ‘Los más buscados’ de Call of Duty: Black Ops 6 (25 de octubre), Ara: History Untold (24 septiembre), Towerborne (acceso anticipado 10 de septiembre). Asimismo, ha compartido nuevos contenidos que llegan a sus principales franquicias. Es el caso de The War Within, que introducirá el 26 de agosto en World of Warcraft la saga The Worldsoul, considerada por Blizzard como “el arco argumental más ambicioso en la historia de WoW”. Starfield, por su parte, se ampliará con la expansión Shattered Space (30 septiembre), con la que se visitará el mundo natal de la Casa Va’ruun, mientras que Diablo IV recibirá el 8 de octubre Vessel of Hatred, una expansión que introduce los mercenarios. Otros títulos anunciados con Masters of Albion, del estudio de desarrollo 22cans; Squid Game, de Netflix; Batman: Arkham Shadow para Meta Quest 3; Herdling, de Okomotive; Marvel Rivals, de Marvel Games y Netease Games; King of Meat, de Glowmade y Amazon Games; y Goat Simulator Remaster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Un juego chino rompió el récord histórico de jugadores simultáneos en Steam con más de dos millones conectados|https://www.lanacion.com.ar/tecnologia/un-juego-chino-rompio-el-record-historico-de-jugadores-simultaneos-en-steam-con-mas-de-dos-millones-nid20082024/|Cuatro años esperaron los usuarios la llegada de Black Myth: Wukong, el videojuego chino de un jugador (single player) basado en la mitología del país. La ansiedad fue tal que este martes, durante su estreno, rompió el récord de mayor cantidad de jugadores simultáneos para un juego de un jugador en Steam. En su pico, llegó a 2.223.179 personas jugando al mismo tiempo. También se posiciona como el más vendido en la plataforma. “Black Myth: Wukong es un RPG de acción inspirado en la mitología china. Encarnarás al Predestinado, que ha de embarcarse en un viaje repleto de peligros y maravillas para descubrir la verdad oculta acerca de una gloriosa leyenda del pasado”, así lo definió su desarrolladora Game Science. El mito corresponde a la Leyenda del Mono y está disponible para Playstation 5 y Xbox. El videojuego tuvo impacto, además, por sus increíbles visuales, que llegaron a ser tendencia en redes sociales. Tal es así que las más de 125 mil reseñas presentes en la plataforma de videojuegos son “extremadamente positivas”. Black Myth: Wukong duplicó el récord del lanzamiento de Cyberpunk 2077 en 2020, donde alcanzó un millón de jugadores concurrentes, y de Palworld, que tuvo 2,1 millones. Aún así, el récord oficial lo tiene el juego de multijugador Playerunknown’s Battlegrounds o PUGB, que tuvo 3,3 millones. Este martes llegó a ser el juego más jugado en Steam, superando a los grandes como el Counter Strike 2, Dota 2 y PUGB. Los halagos al juego se enfocaban en su presentación visual, combate y la construcción de mundos. El primer anuncio del mismo salió en 2020, con un tráiler que tuvo más de cinco millones de visitas en YouTube en el canal del portal de videojuegos y entretenimiento IGN. Desde entonces, los fanáticos esperaron ansiosos su llegada. Game Science no era conocida en ese momento y se llevó la sorpresa de todos. La empresa de desarrollo y publicación de videojuegos proviene de China y es independiente. Tal es así que en 2020 el equipo estaba compuesto por solo 30 personas que venían de trabajar en la empresa tecnológica multinacional Tencent -en los inicios del proyecto de Blackmyth, eran solo siete personas las que trabajaban en su desarrollo en 2018. El juego se basa en la novela china Viaje al Oeste, publicada en el Siglo XVI durante la dinastía Ming. El libro sigue la peregrinación del monje budista Xuanzang de la dinastía Tang, en su viaje para obtener textos sagrados budistas. El videojuego, en cambio, plantea un enfoque alternativo: el jugador juega como el Rey Mono, que acompaña a un monje budista que obtuvo un texto sagrado del Oeste. Aún así, en el tráiler se ven tres monos, y el juego consiste en que el jugador adivine cuál de los tres es el real.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|En el Mundial de Honolulú: un argentino alcanzó el bronce y es el tercer mejor jugador del mundo en Pokémon Go|https://www.lanacion.com.ar/tecnologia/en-el-mundial-de-honolulu-un-argentino-alcanzo-el-bronce-y-quedo-el-tercer-mejor-jugador-del-mundo-nid19082024/ |En un evento masivo, donde más de 3000 participantes en simultáneo disfrutaron de un fin de semana de juegos, la Argentina dejó su marca con decenas de participantes en las disciplinas de videojuegos, Pokémon Go y el famoso juego de cartas en el Pokémon World Championships que se hizo el fin de semana pasado. Durante tres días, jugadores de todo el planeta compitieron en el centro de convenciones de Honolulú, Hawaii, en uno de los eventos de juegos más convocantes del mundo. Allí, entre los 3000 jugadores, cientos de jueces, staff, y visitantes, se destacó la decena de miembros de la “legión argentina de Pokémon” en varias de las disciplinas. Entre ellos la gran actuación de Martín “Martogalde” Galderisi, quien se quedó con el trofeo de bronce, 13.000 dólares de premio y el orgullo de ser el tercer mejor jugador del planeta de Pokémon Go, el juego de realidad aumentada en el que hay que cazar pokémones con el teléfono para entrenarlos y usarlos para desafiar a otros competidores, y que debutó con un éxito inicial inesperado a mediados de 2016. Galderisi llegó invicto a las instancias finales con una gran actuación, que fue transmitida para miles de personas en YouTube y Twitch, como también ante cientos de presentes en el main Stage del Mundial en Hawaii, y quedó en tercer lugar. “Lo que siento es indescriptible”, expresó el argentino, y agregó: “Es algo que venía buscando hace rato. Me había quedado la espina de 2022 de no conseguir un trofeo, y poder llegar al tercer puesto ahora es sin dudas un sueño cumplido. Me hubiese gustado ser campeón, pero estoy inmensamente feliz”. También entre los destacados en el juego de cartas, tuvimos al argentino campeón de 2017 (también en Hawaii), Diego Cassiraga, quien quedó a un paso otra vez de la gloria, tras un complicado empate en las instancias preliminares que lo relegó en puntos y lo dejó en la puerta de las instancias finales. Así lo analizó Cassiraga tras su paso por Honolulú: “Me sentí bien. Es un formato nuevo de torneo, en el que no se puede ni perder. Acá empatar es como perder, pero jugar Pokémon es adaptarse a los cambios y hay que improvisar”. Además, comentó que se viene toda la nueva temporada: “yo aprovecho que viajo con mi familia. Mi hijo de seis años también juega, y este fue su primer mundial junior, y estuvo cerca de llegar al día 2″ y define: “somos una familia Pokémon. Es nuestro hobby y es parte de nuestras vidas”. Chile también tuvo una gran actuación con Fernando Cifuentes, de 17 años, que se coronó campeón en juegos en el juego de cartas, y se destacó el equipo peruano del juego MOBA en equipo Pokémon Unite, Fusion, quien cayó en una semifinal para el infarto, con un tercer puesto en el global que marcó una de las mejores actuaciones de Latinoamérica en la historia del evento. Pero el Pokémon World Championships no solo tuvo su núcleo en el centro de convenciones, sino que vistió a toda la ciudad de Honolulú con eventos, regalos y actividades desplegados por la costa y los centros importantes, incluido el masivo Pokémon Center, de coleccionables únicos, que agotó por completo su stock en los dos primeros días. En la ceremonia de cierre, con bailes, coronaciones y la presencia de los directivos de la franquicia más rentable del mundo, no solo se anunció que el próximo mundial será en Anaheim, California, sino también que el de 2026 será en San Francisco, dejando sellado así la vuelta del mundial a California.|\n",
      "|Cuándo sale el nuevo capítulo de Fornite                                                                      |https://www.lanacion.com.ar/tecnologia/cuando-sale-el-nuevo-capitulo-de-fornite-nid15082024/                                                           |Ya cada vez falta menos para una nueva edición de Battle Royal en Fortnite. De esa forma, da inicio al quinto capítulo de la cuarta temporada centrado en el universo Marvel. Se llama Alerta: Doom y introducirá nuevos skins ―es decir la piel o apariencia de un personaje dentro de un videojuego―, entre ellos el nuevo antagonista de la saga de superhéroes, Doctor Doom. Así se puede observar en el nuevo tráiler que sacó el videojuego. En él se pueden ver varios personajes que conforman el MCU: Capitán América, Máquina de Guerra, Mysterio, Bananerine, Miaúsculos y más. En ese sentido, muchos de sus fanáticos no pueden esperar a que empiece esta nueva etapa de Fornite, que incluirá un mapa con varias referencias a las películas de Marvel. En ese contexto, los usuarios podrán decidir si quieren defender la isla del Doctor Doom y sus secuaces, o si se quieren convertir en villano y unirse al bando antagonista. En la Argentina, se podrá acceder a este nuevo capítulo de Fortnite a partir de la medianoche del viernes 16 de agosto. Es decir, que se podrá jugar a partir de este jueves a la noche. Por el momento, no se sabe hasta cuándo estará disponible. Cabe aclarar es indispensable terminar toda la temporada 3 del capítulo 5 antes de adentrarse a esta nueva etapa del juego. A su vez, es pertinente descargar el parche 31.00, que estará disponible a partir de las 00 de este viernes en nuestro país. Sin embargo, seguro habrá que esperar a que Epic Games habilite nuevamente los servidores tras generar todos los cambios, que por lo general suelen tardar algunas horas. Cómo ocurre en las distintas etapas del Battle Royal, Fornite ofrece nuevos skins para sus jugadores. En este caso, estarán relacionados al universo Marvel, en tono con el nuevo capítulo. A su vez, podrán adquirir nuevas armas para usar en esta batalla. Estos son los nuevos trajes que se pueden desbloquear en Alerta: Doom: Incluso se permitirá usar el skin del propio Doctor Doom, aunque hay que esperar un tiempo para ello. Este traje estará disponible a través de las misiones del pase de batalla en septiembre.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|Doble vida para la consola portátil Asus ROG Ally: podrá correr SteamOS, como la Steam Deck                   |https://www.lanacion.com.ar/tecnologia/doble-vida-para-la-consola-portatil-asus-rog-ally-podra-correr-steamos-como-la-steam-deck-nid15082024/          |Valve ha confirmado que permitirá que otras compañías y plataformas de videojuegos utilicen el sistema operativo que impulsa su consola Steam Deck, SteamOS, como la que fabrica Asus ROG Ally y otros PC. Steam OS es un SO basado en Linux cuya base parte de Debian 8 y que, hasta ahora, es exclusivo de Steam, de manera que no se podía utilizar en equipos desarrollados por otras marcas. Asus, MSI y Lenovo, entre otras, utilizan el sistema operativo de Microsoft, Windows 11. En los últimos meses, Valve ha reconocido que quería trabajar con otros fabricantes para ofrecerles una alternativa basada en SteamOS, tal y como recuerda The Verge. Esto, de hecho, se advirtió en las últimas notas de lanzamiento del SO, en las que se indicaba la “compatibilidad para teclas ROG Ally adicionales”. Ahora, el diseñador de Valve, Lawrence Yang, ha confirmado a este medio que el sistema operativo en el que se basa la consola Steam Deck también se podrá llevar a otras consolas. Entre ellas, la mencionada, fabricada por Asus. Yang también ha indicado que el equipo de Valve “sigue trabajando para añadir compatibilidad con dispositivos portátiles adicionales en SteamOS”, aunque no ha especificado cuáles. No obstante, The Verge recuerda que esto es solo una alternativa para plataformas más allá de Steam Deck y que no significa que Asus vaya a vender su plataforma con el sistema operativo de esta otra compañía. La firma tecnológica taiwanesa ha indicado a este medio que, entre las razones por los que incorpora Windows en su consola, una de las principales es que Microsoft tiene equipos de validación dedicados que garantizan que su sistema operativo funcione en muchas configuraciones de ‘hardware’ y procesadores diferentes. En cualquier caso, Yang ha adelantado que la firma está “haciendo progresos constantes” y que la que sería su nueva versión de SteamOS “aún no está lista” para su lanzamiento.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las primeras filas del DataFrame\n",
    "print(\"Primeras filas del DataFrame:\")\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813ff0e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:00.297114Z",
     "iopub.status.busy": "2024-08-27T17:59:00.296876Z",
     "iopub.status.idle": "2024-08-27T17:59:00.301496Z",
     "shell.execute_reply": "2024-08-27T17:59:00.300934Z"
    },
    "papermill": {
     "duration": 0.009066,
     "end_time": "2024-08-27T17:59:00.302508",
     "exception": false,
     "start_time": "2024-08-27T17:59:00.293442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esquema del DataFrame:\n",
      "root\n",
      " |-- titulo: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- cuerpo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el esquema del DataFrame (tipos de datos de cada columna)\n",
    "print(\"\\nEsquema del DataFrame:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfec39d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:00.308809Z",
     "iopub.status.busy": "2024-08-27T17:59:00.308541Z",
     "iopub.status.idle": "2024-08-27T17:59:01.351038Z",
     "shell.execute_reply": "2024-08-27T17:59:01.350323Z"
    },
    "papermill": {
     "duration": 1.047592,
     "end_time": "2024-08-27T17:59:01.352677",
     "exception": false,
     "start_time": "2024-08-27T17:59:00.305085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descripción estadística de las columnas numéricas:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|summary|              titulo|                link|              cuerpo|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|  count|                1916|                1917|                1912|\n",
      "|   mean|                NULL|                NULL|                NULL|\n",
      "| stddev|                NULL|                NULL|                NULL|\n",
      "|    min|          \"\"\"Bury Me|           My Love\"\"|    dice su creador\"|\n",
      "|    max|“Tenemos paciente...|https://www.lanac...|“¡Pon a remojar l...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describir las estadísticas básicas\n",
    "print(\"\\nDescripción estadística de las columnas numéricas:\")\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ffc734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:01.360040Z",
     "iopub.status.busy": "2024-08-27T17:59:01.359775Z",
     "iopub.status.idle": "2024-08-27T17:59:01.651716Z",
     "shell.execute_reply": "2024-08-27T17:59:01.650390Z"
    },
    "papermill": {
     "duration": 0.299029,
     "end_time": "2024-08-27T17:59:01.655197",
     "exception": false,
     "start_time": "2024-08-27T17:59:01.356168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de valores nulos por columna:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|titulo|link|cuerpo|\n",
      "+------+----+------+\n",
      "|     1|   0|     5|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay valores nulos\n",
    "print(\"\\nNúmero de valores nulos por columna:\")\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab480d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:01.675325Z",
     "iopub.status.busy": "2024-08-27T17:59:01.674930Z",
     "iopub.status.idle": "2024-08-27T17:59:01.959995Z",
     "shell.execute_reply": "2024-08-27T17:59:01.959317Z"
    },
    "papermill": {
     "duration": 0.296394,
     "end_time": "2024-08-27T17:59:01.961285",
     "exception": false,
     "start_time": "2024-08-27T17:59:01.664891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros con valores nulos en alguna columna:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|titulo                                                                    |link                                                                                                                                                            |cuerpo                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|NULL                                                                      |https://www.lanacion.com.ar/deportes/canchallena/fifa-23-el-trailer-oficial-cuando-es-el-lanzamiento-los-nuevos-torneos-y-todo-lo-que-hay-que-saber-nid20072022/|EA Sports presentó el tráiler oficial del FIFA 23, el último bajo este nombre, y se conocieron varios detalles en la antesala del lanzamiento. Por primera vez, el juego estará cruzado con varios detalles relacionados a la expansión del fútbol femenino a nivel global: Sam Kerr estará en la portada del juego, habrá clubes disponibles de dos ligas, tendrá la posibilidad de jugar el Mundial 2023 y sumará nuevos detalles de animación.|\n",
      "|\"Carlos Pérez: \"\"Comunicación y modelo de negocio deberían ser lo mismo\"\"\"|https://www.lanacion.com.ar/economia/carlos-perez-comunicacion-y-modelo-de-negocio-deberian-ser-lo-mismo-nid1976919/                                            |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Los Angeles: qué novedades trae el FIFA 2017                              |https://www.lanacion.com.ar/tecnologia/los-angeles-que-novedades-trae-el-fifa-2017-nid1908907/                                                                  |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Todo el color de la Tokyo Game Show, la mega feria de videojuegos         |https://www.lanacion.com.ar/tecnologia/todo-el-color-de-la-tokyo-game-show-la-mega-feria-de-videojuegos-nid1621754/                                             |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Eligen los mejores videojuegos de la historia                             |https://www.lanacion.com.ar/tecnologia/eligen-los-mejores-videojuegos-de-la-historia-nid1528816/                                                                |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Probamos la consola Sony PS Vita                                          |https://www.lanacion.com.ar/tecnologia/probamos-la-sony-ps-vita-nid1482479/                                                                                     |NULL                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+--------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar los registros que tienen valores nulos en alguna columna\n",
    "df_with_nulls = df.filter(F.greatest(*[F.col(c).isNull().cast(\"int\") for c in df.columns]) == 1)\n",
    "\n",
    "# Mostrar los registros con nulos\n",
    "print(\"Registros con valores nulos en alguna columna:\")\n",
    "df_with_nulls.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd251d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:01.969240Z",
     "iopub.status.busy": "2024-08-27T17:59:01.968919Z",
     "iopub.status.idle": "2024-08-27T17:59:01.983153Z",
     "shell.execute_reply": "2024-08-27T17:59:01.982474Z"
    },
    "papermill": {
     "duration": 0.019607,
     "end_time": "2024-08-27T17:59:01.984576",
     "exception": false,
     "start_time": "2024-08-27T17:59:01.964969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eliminar los registros que tienen algún valor nulo\n",
    "df_cleaned = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34292420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:01.991865Z",
     "iopub.status.busy": "2024-08-27T17:59:01.991545Z",
     "iopub.status.idle": "2024-08-27T17:59:02.205567Z",
     "shell.execute_reply": "2024-08-27T17:59:02.204804Z"
    },
    "papermill": {
     "duration": 0.219286,
     "end_time": "2024-08-27T17:59:02.206799",
     "exception": false,
     "start_time": "2024-08-27T17:59:01.987513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros con valores nulos en alguna columna:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|titulo|link|cuerpo|\n",
      "+------+----+------+\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar los registros que tienen valores nulos en alguna columna\n",
    "df_with_nulls = df_cleaned.filter(F.greatest(*[F.col(c).isNull().cast(\"int\") for c in df_cleaned.columns]) == 1)\n",
    "\n",
    "# Mostrar los registros con nulos\n",
    "print(\"Registros con valores nulos en alguna columna:\")\n",
    "df_with_nulls.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74618e5e",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd171923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T17:59:02.214300Z",
     "iopub.status.busy": "2024-08-27T17:59:02.213973Z",
     "iopub.status.idle": "2024-08-27T17:59:03.542077Z",
     "shell.execute_reply": "2024-08-27T17:59:03.539836Z"
    },
    "papermill": {
     "duration": 1.334445,
     "end_time": "2024-08-27T17:59:03.544375",
     "exception": true,
     "start_time": "2024-08-27T17:59:02.209930",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas con puntuación de sentimiento:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 14:59:02 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 17)\n",
      "org.apache.spark.SparkException: \n",
      "Error from python worker:\n",
      "  Traceback (most recent call last):\n",
      "    File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "    File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 148, in <module>\n",
      "    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/__init__.py\", line 42, in <module>\n",
      "    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 28, in <module>\n",
      "      \n",
      "      ^\n",
      "    File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 8, in <module>\n",
      "      from _ctypes import Union, Structure, Array\n",
      "  ModuleNotFoundError: No module named '_ctypes'\n",
      "PYTHONPATH was:\n",
      "  /home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\n",
      "org.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout.\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:246)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r",
      "24/08/27 14:59:02 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 17) (mauro-thinkpad.fibertel.com.ar executor driver): org.apache.spark.SparkException: \n",
      "Error from python worker:\n",
      "  Traceback (most recent call last):\n",
      "    File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "    File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 148, in <module>\n",
      "    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/__init__.py\", line 42, in <module>\n",
      "    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 28, in <module>\n",
      "      \n",
      "      ^\n",
      "    File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 8, in <module>\n",
      "      from _ctypes import Union, Structure, Array\n",
      "  ModuleNotFoundError: No module named '_ctypes'\n",
      "PYTHONPATH was:\n",
      "  /home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\n",
      "org.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout.\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:246)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "24/08/27 14:59:02 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o103.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 17) (mauro-thinkpad.fibertel.com.ar executor driver): org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"<frozen runpy>\", line 189, in _run_module_as_main\n    File \"<frozen runpy>\", line 112, in _get_module_details\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 148, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/__init__.py\", line 42, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 28, in <module>\n      \n      ^\n    File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 8, in <module>\n      from _ctypes import Union, Structure, Array\n  ModuleNotFoundError: No module named '_ctypes'\nPYTHONPATH was:\n  /home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout.\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:246)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"<frozen runpy>\", line 189, in _run_module_as_main\n    File \"<frozen runpy>\", line 112, in _get_module_details\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 148, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/__init__.py\", line 42, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 28, in <module>\n      \n      ^\n    File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 8, in <module>\n      from _ctypes import Union, Structure, Array\n  ModuleNotFoundError: No module named '_ctypes'\nPYTHONPATH was:\n  /home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout.\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:246)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Mostrar algunas filas con la nueva columna de sentimiento\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimeras filas con puntuación de sentimiento:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdf_with_sentiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o103.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 17) (mauro-thinkpad.fibertel.com.ar executor driver): org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"<frozen runpy>\", line 189, in _run_module_as_main\n    File \"<frozen runpy>\", line 112, in _get_module_details\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 148, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/__init__.py\", line 42, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 28, in <module>\n      \n      ^\n    File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 8, in <module>\n      from _ctypes import Union, Structure, Array\n  ModuleNotFoundError: No module named '_ctypes'\nPYTHONPATH was:\n  /home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout.\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:246)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"<frozen runpy>\", line 189, in _run_module_as_main\n    File \"<frozen runpy>\", line 112, in _get_module_details\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 148, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/__init__.py\", line 42, in <module>\n    File \"/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 28, in <module>\n      \n      ^\n    File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 8, in <module>\n      from _ctypes import Union, Structure, Array\n  ModuleNotFoundError: No module named '_ctypes'\nPYTHONPATH was:\n  /home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip:/home/mauro/.local/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout.\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:246)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Definir una función para calcular el sentimiento usando TextBlob\n",
    "def get_sentiment(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Registrar la función como una UDF (User Defined Function)\n",
    "sentiment_udf = F.udf(get_sentiment)\n",
    "\n",
    "# Crear la columna 'sentiment' basada en el cuerpo de la noticia\n",
    "df_with_sentiment = df.withColumn(\"sentiment\", sentiment_udf(F.col(\"cuerpo\")))\n",
    "\n",
    "# Mostrar algunas filas con la nueva columna de sentimiento\n",
    "print(\"Primeras filas con puntuación de sentimiento:\")\n",
    "df_with_sentiment.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f95e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_sentiment.write.mode(\"overwrite\").csv(\"../data/processed/news.csv\", header=True, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108876d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Asumiendo que df_with_sentiment ya está definido y contiene los datos más recientes\n",
    "\n",
    "# Paso 1: Monitoreo de Sentimiento por Fecha\n",
    "df_with_sentiment = df_with_sentiment.withColumn('fecha', F.to_date(F.col('fecha_column_name'), 'yyyy-MM-dd'))\n",
    "sentiment_by_date = df_with_sentiment.groupBy('fecha').agg(F.avg('sentiment').alias('avg_sentiment')).orderBy('fecha')\n",
    "sentiment_pd = sentiment_by_date.toPandas()\n",
    "sentiment_pd = sentiment_pd.sort_values('fecha')\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=sentiment_pd, x='fecha', y='avg_sentiment', marker='o')\n",
    "plt.title('Evolución del Sentimiento Promedio de Noticias de Videojuegos')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Sentimiento Promedio')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sentiment_trend.png')  # Guarda la gráfica como una imagen\n",
    "plt.close()\n",
    "\n",
    "# Paso 10: Sistema de Alerta\n",
    "threshold = 0.2\n",
    "sentiment_pd['sentiment_change'] = sentiment_pd['avg_sentiment'].diff()\n",
    "latest_change = sentiment_pd.iloc[-1]['sentiment_change']\n",
    "latest_date = sentiment_pd.iloc[-1]['fecha']\n",
    "\n",
    "if latest_change <= -threshold:\n",
    "    print(f\"ALERTA: El sentimiento promedio cayó {abs(latest_change):.2f} puntos el día {latest_date}.\")\n",
    "    # Aquí podrías añadir código para enviar un correo electrónico o una notificación\n",
    "else:\n",
    "    print(f\"El sentimiento promedio está estable el día {latest_date}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.689005,
   "end_time": "2024-08-27T17:59:06.175428",
   "environment_variables": {},
   "exception": true,
   "input_path": "python-etl/transform_data.ipynb",
   "output_path": "python-etl/output_notebook.ipynb",
   "parameters": {},
   "start_time": "2024-08-27T17:58:51.486423",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}